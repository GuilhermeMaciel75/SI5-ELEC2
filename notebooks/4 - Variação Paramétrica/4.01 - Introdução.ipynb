{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Fase 4 - Variação paramétrica**\n",
    "\n",
    "Nesta fase do projeto, intitulada \"Variação Paramétrica\", damos continuidade ao processo de modelagem preconizado pelo framework CRISP-DM, com foco na avaliação e otimização de diversos modelos de aprendizado de máquina aplicados ao dataset ELEC2. O objetivo principal é investigar como diferentes combinações de hiperparâmetros impactam o desempenho dos modelos, garantindo a escolha mais adequada para o problema de classificação em análise. Ao longo do texto, exploraremos tanto modelos clássicos como KNN, Árvore de Decisão e SVM, quanto técnicas mais avançadas como Random Forest, XGBoost, LightGBM, redes neurais MLP e comitês (ensemble models), além de detalhar a metodologia de testes utilizada e os critérios adotados para avaliação dos resultados. A análise busca equilibrar desempenho, custo computacional e robustez frente à variação nos dados, considerando aspectos como deriva de conceito, overfitting e estabilidade dos modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Técnica de Modelagem\n",
    "\n",
    "Para essa etapa do CRISP-DM, utilizaremos o dataset base gerado na fase 3, e os modelos selecionados na fase 1 junto ao stakeholder. Durante a fase de modelagem, utilizaremos os seguintes modelos:\n",
    "\n",
    "- K-NN (K-Nearest Neighbors)\n",
    "- LVQ (Learning Vector Quantization):\n",
    "- Árvore de Decisão:\n",
    "- SVM (Support Vector Machine)\n",
    "- Random Forest\n",
    "- Rede Neural MLP (Multilayer Perceptron)\n",
    "- Comitê de Redes Neurais Artificiais\n",
    "- Comitê Heterogêneo (Stacking)\n",
    "- XGBoost\n",
    "- LightGBM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Suposições da Modelagem\n",
    "\n",
    "Modelos como o **K-NN (K-Nearest Neighbors)**, por exemplo, podem sofrer com a deriva de conceito devido à sua dependência das instâncias mais próximas no espaço de características. Como o K-NN não leva em consideração o histórico das mudanças nos dados, ele pode ter dificuldades em se adaptar quando a distribuição de consumo de energia muda ao longo do tempo, especialmente em casos atípicos na variação do consumo e gasto energético, resultando em uma performance instável. Dessa forma, o desempenho desse modelo será muito dependente da escolha do valor de K e, ainda assim, não acreditamos que esse modelo terá o melhor desempenho em comparação com outras possibilidades devido a sua simplicidade.\n",
    "\n",
    "Já o **LVQ (Learning Vector Quantization)**, um modelo de rede neural supervisionada, pode ser uma boa escolha para lidar com a separação em clusters, pois é projetado para identificar regiões de decisão e realizar classificações baseadas nessas regiões. No entanto, assim como o K-NN, o LVQ pode ser sensível à deriva de conceito e a casos atípicos (outliers), especialmente se as mudanças nos dados forem substanciais. Apesar disso, devido à dificuldade em encontrar bibliotecas que implementem esse modelo, e à necessidade de utilizar o algoritmo visto em sala, que não é muito eficiente no treinamento, acreditamos que ele também não apresentará resultados satisfatórios, devido ao alto tempo de treinamento e ao baixo desempenho nos testes iniciais.\n",
    "\n",
    "A **Árvore de Decisão**, por sua vez, é mais resistente à deriva do que os modelos baseados em vizinhos, pois realiza divisões hierárquicas nos dados com base nas variáveis mais relevantes, se preocupando em encontrar valores que realizem divisões entre as classes. Como não há uma mudança significativa na distribuição de dados do modelo, acreditamos que ela poderá ter um desempenho aceitável. Entretanto, a falta de robustez do modelo e a complexidade do problema podem dificultar a predição da classe alvo. Por fim, outro problema que ela pode enfrentar é o **overfitting**, capturando ruídos em vez de padrões generalizáveis.\n",
    "\n",
    "No caso do **SVM (Support Vector Machine)**, o modelo é eficaz na criação de margens de separação, principalmente em problemas de classificação não linear. Contudo, a sua eficácia no ELEC2 pode ser afetada pela necessidade de ajuste do kernel e pela regularização. Assim a SVM pode ser muito eficaz para capturar padrões de consumo de energia em períodos de estabilidade, mas à medida que o mercado muda e a distribuição dos dados varia, a precisão do modelo pode ser comprometida, exigindo atualizações constantes. Ainda assim, não acreditamos que ela terá um bom desempenho para o conjunto de dados, dependendo bastante da busca de hiperpârametros.\n",
    "\n",
    "O **Random Forest**, por ser um modelo ensemble baseado em múltiplas árvores de decisão, tem a vantagem de ser mais robusto em relação à deriva de conceito, logo provavelmente deverá apresentar um resultado melhor que ele. Ao combinar previsões de várias árvores, ele é menos propenso a se ajustar excessivamente a padrões temporários ou ruidosos, o que o torna uma boa escolha para lidar com a variabilidade e com mudanças no comportamento de consumo de energia. Ainda assim, sua capacidade de **generalização** é amplamente estudada e utilizada na literatura, e o ajuste adequado dos hiperparâmetros pode proporcionar um bom equilíbrio entre complexidade e precisão. Acreditamos que o Random Forest será um modelo que irá desempenhar muito bem nas métricas de avaliação e deverá ser um dos modelos avaliados na escolha final, uma vez que foi um modelo que apresentou resultados bastante satisfatórios em artigos que utilizam essa base\n",
    "\n",
    "Modelos mais complexos, como a **Rede Neural MLP (Multilayer Perceptron)**, podem oferecer uma boa capacidade de modelar relações não lineares complexas entre as variáveis do dataset. No entanto, as redes neurais podem ser sensíveis a deriva, especialmente se não forem treinadas de forma contínua. Ainda assim, elas podem se tornar desatualizadas ao longo do tempo, caso não sejam ajustadas para capturar novas mudanças nos dados. Por fim, acreditamos que o MLP apresentará um alto desempenho, porém esse alto desempenho trará um alto **tempo de treinamento** e um alto custo de busca de hiperparâmetros.\n",
    "\n",
    "O **Comitê de Redes Neurais Artificiais**, que combina várias redes neurais para melhorar a robustez e a precisão, pode ser uma solução eficaz para enfrentar a deriva de conceito. Ao combinar diferentes redes, o comitê pode se beneficiar da diversidade de aprendizado, resultando em uma maior precisão, especialmente se combinado com técnicas de bagging ou boosting. Contudo, maximiza ainda mais o problema apresentado em MLP, uma vez que com o uso de mais de um algoritmo de MLP trará um alto custo para encontrar a melhor combinação entre essas redes neurais, além de trazer um custo computacional maior, uma vez que será necessário executar mais de um modelo desse ao mesmo tempo durante as previsões \n",
    "\n",
    "O **Comitê Heterogêneo (Stacking)**, por combinar modelos de diferentes tipos, oferece uma abordagem poderosa para lidar com dados que apresentam variabilidade temporal, como no caso do ELEC2. Ao usar uma combinação de modelos como SVM, Árvores de Decisão e Redes Neurais, o stacking pode melhorar a precisão ao capturar diferentes aspectos dos dados e suas interações, além de ser capaz de se adaptar às mudanças ao longo do tempo. Ainda assim, esse desempenho só será possível se utilizarmos bons modelos como base para esse comitê, como iremos utilizar modelos mais simples, não sabemos ao certo como será o desempenho dessa estratégia. Por fim, devido a esse desafio de modelagem do comitê e a necessidade de utilizar mais de um modelo ao mesmo tempo, acreditamos que ele irá possuir um bom desempenho, mas não tão eficiente frente a outras estratégias como Random Forest e XGboost.\n",
    "\n",
    "O **XGBoost** e o **LightGBM**, ambos baseados em técnicas de boosting, são modelos altamente eficazes para tarefas de classificação e regressão. O XGBoost, em particular, é um dos modelos mais populares e robustos para problemas complexos e altamente utilizado na literatura, como o ELEC2. Ele lida bem com dados desbalanceados, além de ser menos sensível a overfitting quando configurado corretamente. O LightGBM, por ser mais eficiente em termos de memória e tempo de treinamento, também é uma excelente escolha para datasets grandes e com variabilidade. Assim, ambos os modelos são robustos à deriva de conceito e podem ser ajustados facilmente para se adaptarem às mudanças nos dados de consumo de energia. Por fim, acreditamos que esses modelos apresentaram um alto desempenho e com certeza estarão na ponderação do modelo final.\n",
    "\n",
    "Dentre as opções analisadas, o **Random Forest** e o **XGBoost** se destacam como as escolhas mais promissoras para o conjunto de dados do ELEC2, considerando sua robustez à deriva de conceito, sua capacidade de lidar com a variabilidade dos dados e a facilidade de ajustes para otimizar o desempenho. A escolha final dependerá da complexidade e do equilíbrio desejado entre desempenho e custo computacional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Design de Teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - Design de Execução dos Testes\n",
    "\n",
    "Nesta etapa, implementamos uma bancada de testes padronizada que será utilizada em todos os modelos para garantir que os testes sejam realizados de forma isonômica. Isso assegura que as condições de execução dos modelos sejam consistentes, permitindo comparações justas entre os diferentes modelos e hiperparâmetros.\n",
    "\n",
    "A estrutura do teste é gerida por uma função que recebe como parâmetros o modelo a ser avaliado, o conjunto de hiperparâmetros a serem investigados, e o conjunto de dados de treinamento, validação e teste. Esses parâmetros são usados para alimentar um **RandomizedSearchCV**, uma ferramenta da biblioteca scikit-learn que realiza a busca de hiperparâmetros de forma eficiente. O **RandomizedSearchCV** é particularmente útil quando há uma grande quantidade de possíveis combinações de hiperparâmetros, pois permite testar diferentes combinações de forma aleatória e rápida, sem a necessidade de uma busca exaustiva.\n",
    "\n",
    "Realizamos a execução do **RandomizedSearchCV** com 20 iterações, e em cada iteração, 20 combinações de parâmetros são selecionadas aleatoriamente. Esta abordagem permite uma avaliação mais diversa e uma exploração mais ampla do espaço de hiperparâmetros.\n",
    "\n",
    "Para assegurar que a combinação de hiperparâmetros gerada seja confiável, robusta e tenha boa capacidade de generalização, utilizamos a validação cruzada **K-fold** com $k = 5$. Isso significa que os dados de treinamento são divididos em 5 subconjuntos, e o modelo é treinado e validado em cada uma dessas divisões. Com isso, obtemos um total de 100 ajustes por iteração, o que resulta em 2.000 ajustes ao final de todas as iterações para um único modelo.\n",
    "\n",
    "Para a seleção do melhor modelo dentro do **RandomizedSearchCV**, utilizamos como critério principal a métrica **roc_auc**. Sua escolha se dá por sua robustez em avaliar o desempenho de classificadores, especialmente quando lidamos com classes desbalanceadas (Embora estejamos trabalhando em um problema em que as classes estão relativamente balanceadas). Essa métrica considera tanto a taxa de verdadeiros positivos quanto a taxa de falsos positivos, fornecendo uma avaliação completa da capacidade de discriminação do modelo.\n",
    "\n",
    "Após a execução do **RandomizedSearchCV**, o melhor modelo é selecionado, e realizamos testes adicionais para avaliar sua acurácia nos dados de treinamento, validação e teste. As métricas avaliadas incluem **acurácia**, **auc**, **f1_score**, **recall**, **true positive rate**, **false positive rate**, além do tempo de ajuste do modelo. Também realizamos uma análise detalhada do desempenho dos 20 melhores modelos obtidos, avaliando seu desempenho médio durante a validação cruzada. Essa análise nos permite entender melhor quais parâmetros representam o modelo de forma mais eficaz e quais estratégias podem ser adotadas para otimizar o desempenho geral.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from lib.util import RANDOM_STATE, load_json, save_json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_curve, roc_auc_score, recall_score\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def search_paramsv2(model, params, model_name, dataset, n_iter=20, score='auc', verbose=0, save=True):\n",
    "    \n",
    "    MODEL_PATH = f'./models/{model_name}'\n",
    "    \n",
    "    if os.path.exists(MODEL_PATH) and not save:\n",
    "        best_params = load_json(f\"{MODEL_PATH}/{model_name}.json\")\n",
    "\n",
    "        with open(f\"{MODEL_PATH}/{model_name}.pkl\", 'wb') as file:\n",
    "            best_model = pickle.load(file)\n",
    "        \n",
    "        best_cv = pd.read_csv(f\"{MODEL_PATH}/best_cv.csv\")\n",
    "        mean_scores = pd.read_csv(f\"{MODEL_PATH}/mean_scores.csv\")\n",
    "        \n",
    "        return best_params, best_model, best_cv, None, mean_scores\n",
    "        \n",
    "    \n",
    "    X_train, Y_train, X_val, Y_val, X_test, Y_test = map(np.array, dataset)\n",
    "    \n",
    "    best_params = {}\n",
    "    best_model = None\n",
    "    best_score = -float('inf')\n",
    "    best_cv = None\n",
    "    all_cv = pd.DataFrame()\n",
    "    \n",
    "    \n",
    "    for i in tqdm(range(20), desc=\"Realizando a Busca de Parâmetros por 20 iterações\"):\n",
    "        \n",
    "        search_model = RandomizedSearchCV(\n",
    "            estimator=model,\n",
    "            param_distributions=params,\n",
    "            cv=5,\n",
    "            scoring='roc_auc',\n",
    "            n_iter=n_iter,\n",
    "            random_state=RANDOM_STATE + i,\n",
    "            n_jobs=-1,\n",
    "            refit=True,\n",
    "            return_train_score=True,\n",
    "            verbose=verbose\n",
    "        )\n",
    "        \n",
    "        search_model.fit(X_train, Y_train)\n",
    "        \n",
    "        all_cv = pd.concat([all_cv, pd.DataFrame(search_model.cv_results_).iloc[search_model.best_index_]], axis=1)\n",
    "                \n",
    "        Y_pred_train = search_model.best_estimator_.predict(X_train)\n",
    "        Y_pred_val = search_model.best_estimator_.predict(X_val)\n",
    "        Y_pred_test = search_model.best_estimator_.predict(X_test)\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(Y_test, Y_pred_test)\n",
    "            \n",
    "        best_params[i] = {\n",
    "            \"params\" : search_model.best_params_,\n",
    "            \"score\" : search_model.best_score_,\n",
    "            \"model_time\" : search_model.refit_time_,\n",
    "            \n",
    "            'accuracy_train': accuracy_score(Y_train, Y_pred_train),\n",
    "            'accuracy_val' : accuracy_score(Y_val, Y_pred_val),\n",
    "            'accuracy_test': accuracy_score(Y_test, Y_pred_test),\n",
    "            \n",
    "            'f1_score': f1_score(Y_test, Y_pred_test),\n",
    "            'recall': recall_score(Y_test, Y_pred_test),\n",
    "            'auc': roc_auc_score(Y_test, Y_pred_test),\n",
    "            \n",
    "            'fpr': list(fpr),\n",
    "            'tpr': list(tpr),\n",
    "            \n",
    "            \"model_path\" : f\"./models/{model_name}/{i}.pkl\"\n",
    "        }\n",
    "        \n",
    "        if best_params[i].get(score) > best_score:\n",
    "            best_score = best_params[i].get(score)\n",
    "            best_model = search_model.best_estimator_\n",
    "            best_cv = pd.DataFrame(search_model.cv_results_).iloc[search_model.best_index_]\n",
    "            best_loss = search_model.best_estimator_.loss_curve_ \\\n",
    "                if hasattr(search_model.best_estimator_, \"loss_curve_\") else None\n",
    "\n",
    "        \n",
    "        if not os.path.exists(MODEL_PATH):\n",
    "            os.makedirs(MODEL_PATH)\n",
    "        \n",
    "    \n",
    "    with open(f\"{MODEL_PATH}/{model_name}.pkl\", 'wb') as file:\n",
    "            pickle.dump(search_model.best_estimator_, file)\n",
    "    \n",
    "    best_cv.to_csv(f\"{MODEL_PATH}/best_cv.csv\")\n",
    "    \n",
    "    save_json(best_params, f\"{MODEL_PATH}/{model_name}.json\")\n",
    "    \n",
    "    rows_to_select = [row for row in all_cv.index if 'split' in row and 'score' in row]\n",
    "    subset_cv = all_cv.loc[rows_to_select]\n",
    "    subset_cv\n",
    "    mean_scores = subset_cv.mean(axis=1)\n",
    "    mean_scores.to_csv(f\"{MODEL_PATH}/mean_scores.csv\")\n",
    "        \n",
    "    return best_params, best_model, best_cv, best_loss, mean_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Design da Avaliação dos Parâmetros\n",
    "\n",
    "Como discutido anteriormente, o modelo selecionado como o melhor é aquele que apresenta o maior **roc_auc** durante o processo de busca de hiperparâmetros. No entanto, para garantir uma avaliação mais completa e robusta dos parâmetros, testamos diversos outros parâmetros e utilizando uma série de plots para analisar o comportamento do modelo sob diferentes perspectivas.\n",
    "\n",
    "### 2.2.1 Curva ROC do Melhor Parâmetro x Médias do Modelo\n",
    "\n",
    "Este gráfico tem como objetivo não apenas avaliar a confiabilidade do modelo (analisando a curva ROC), mas também verificar sua estabilidade. A curva ROC do melhor modelo selecionado é comparada com a média das curvas ROC dos 20 melhores modelos. Isso nos permite identificar se o modelo selecionado apresenta um desempenho significativamente superior e estável, ou se a variação entre os modelos é excessiva, o que poderia indicar problemas de overfitting ou instabilidade.\n",
    "\n",
    "### 2.2.2 Matriz de Confusão do Melhor Estimador do Modelo\n",
    "\n",
    "A matriz de confusão oferece uma visão detalhada do desempenho do modelo ao classificar os dados em diferentes categorias. Esta análise permite avaliar a quantidade de verdadeiros positivos, falsos positivos, verdadeiros negativos e falsos negativos, sendo essencial para compreender os erros cometidos pelo modelo. Com ela, conseguimos analisar se o modelo tende a favorecer certas classes ou se está cometendo erros sistemáticos.\n",
    "\n",
    "### 2.2.3 Comparação das Métricas Médias x Máximo\n",
    "\n",
    "Este gráfico compara a **média das métricas** de cada modelo com o **valor máximo alcançado por algum modelo em cada métrica**. A ideia aqui é avaliar o quão distante a média dos modelos fica do valor máximo observado em cada métrica. Isso permite verificar se existe uma grande variação entre os modelos, ou se muitos modelos apresentam um desempenho similar, com o valor máximo representando o melhor resultado possível dentro daquele conjunto de parâmetros. Esse gráfico ajuda a entender a distribuição dos desempenhos e a consistência dos modelos, destacando quais métricas têm uma maior dispersão e quais apresentam maior estabilidade nos resultados.\n",
    "\n",
    "### 2.2.4 Comparação das Métricas para Cada Estimador\n",
    "\n",
    "Este gráfico apresenta as métricas de desempenho de cada estimador, representadas ao longo do eixo X, com valores de desempenho no eixo Y. As linhas representam diferentes métricas de avaliação como **acurácia**, **f1_score**, **recall**, e **auc**, e são agrupadas para os dados de treinamento, validação e teste. A análise desse gráfico permite avaliar como as métricas variam entre os diferentes parâmetros selecionados e suas iterações, facilitando a comparação entre eles e a escolha do melhor conjunto de parâmetros.\n",
    "\n",
    "### 2.2.5 Comparação do Melhor Estimador Durante a Cross Validation\n",
    "\n",
    "Neste gráfico, apresentamos os valores dos scores **roc_auc** obtidos durante a execução da validação cruzada. Esse gráfico oferece uma visão de como o modelo de melhor desempenho se comporta ao longo das divisões dos dados de treinamento e teste, permitindo avaliar a consistência do modelo e seu desempenho em diferentes subconjuntos dos dados.\n",
    "\n",
    "### 2.2.6 Comparação do Desempenho Médio dos Melhores Estimadores Durante a Cross Validation\n",
    "\n",
    "Esse gráfico mostra o **desempenho médio** dos melhores estimadores ao longo das divisões da validação cruzada. Ele é fundamental para entender a consistência do desempenho dos modelos, fornecendo uma visão global de como os modelos se comportam em diferentes cenários, o que pode ajudar na seleção do modelo com maior capacidade de generalização."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
